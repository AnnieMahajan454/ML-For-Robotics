# ML-For-Robotics

This repository contains coursework and experiments completed as part of the **ML for Robotics Lab**. Each lab focuses on building practical understanding of machine learning workflows.


## Lab 1 – Data Understanding & Variability

This lab focuses on foundational data exploration and statistical variability analysis. It covers computation of summary statistics (mean, median, mode), dispersion measures (variance, standard deviation, IQR), and visualization techniques for identifying outliers and distribution characteristics. Datasets such as *tips.csv*, *Vanilla.csv*, and *StudentsPerformance.csv* were used to develop intuition about data spread and preprocessing prior to modeling.


## Lab 2 – Linear Regression

This experiment introduces supervised learning through linear regression modeling. It includes implementing simple and multiple linear regression, visualizing regression fits, and evaluating performance using metrics such as Mean Squared Error and R² score. The lab emphasizes understanding relationships between dependent and independent variables and interpreting coefficients in predictive modeling contexts.


## Lab 3 – Naive Bayes Classification

This lab explores probabilistic classification using the Naive Bayes algorithm. It covers data preprocessing, feature independence assumptions, training classifiers on labeled datasets, and evaluating predictions through confusion matrices and accuracy metrics. The exercise demonstrates how Bayesian reasoning can be applied efficiently for classification problems.


## Lab 4 – Decision Trees

This experiment focuses on decision tree learning for classification tasks. Topics include entropy and Gini impurity, tree construction, splitting criteria, and visualization of learned decision boundaries. The lab highlights interpretability of tree-based models and discusses overfitting control through depth limitation and pruning strategies.


## Lab 5 – Hyperparameters

This lab investigates hyperparameter tuning for decision tree models to improve generalization performance. Techniques such as k-fold cross-validation, grid search, and randomized search are applied to systematically evaluate parameter combinations. The experiment demonstrates how tuning parameters like tree depth and split thresholds directly influences model bias–variance tradeoffs.


## Lab 6 – KNN, Logistic Regression, AUC & ROC

This experiment compares multiple classification approaches including k-Nearest Neighbors and Logistic Regression. It introduces model evaluation beyond accuracy through ROC curves and AUC metrics to assess classifier discrimination capability. The lab strengthens understanding of distance-based learning, probabilistic classification, and threshold-independent evaluation techniques.

## Lab 7 – Single Layer & Multi-Layer Perceptrons (SLP/MLP)

This lab explores neural network fundamentals through the implementation of Single Layer and Multi-Layer Perceptrons. It covers perceptron learning for logical gate problems, visualization of decision boundaries, and training of MLP models for nonlinear classification tasks. Experiments include activation function comparisons, performance evaluation using real datasets, and analysis of training loss to understand convergence behavior and model capacity.

## Lab 8 – Hyperparameter Tuning (MLP & Linear SVM)

This lab focuses on optimizing machine learning models through hyperparameter tuning. It explores Stochastic Gradient Descent (SGD) in MLP, grid search for automatic parameter selection, and implementation of Linear SVM for classification tasks. Experiments are performed on datasets such as Iris, Wisconsin Breast Cancer, Titanic, and Indian Diabetes to evaluate model performance and understand the impact of parameters like learning rate, regularization, and hidden layer configuration.

## Repository Structure

Each lab directory includes:

* Source code implementations
* Supporting datasets or references
* Output visualizations and evaluation results


## Tools & Libraries

* Python
* NumPy
* Pandas
* Matplotlib / Seaborn
* Scikit-learn


## Learning Outcomes

Through these experiments, the repository demonstrates practical competence in:

* Data exploration and preprocessing
* Regression and classification modeling
* Model evaluation techniques
* Hyperparameter optimization
* Comparative algorithm analysis

